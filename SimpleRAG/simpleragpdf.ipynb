{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (3.4.1)\n",
      "Requirement already satisfied: langchain in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (0.2.6)\n",
      "Requirement already satisfied: faiss-cpu in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (1.10.0)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (1.35.5)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (4.49.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from sentence-transformers) (2.3.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from sentence-transformers) (0.29.3)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (0.2.10)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (0.1.82)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (2.7.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (8.4.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install sentence-transformers langchain faiss-cpu openai transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load a PDF file or Webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Q u a l c o m m  G e n A I  P r e p a r a t i o n\\nM o d e l  O p t i m i z a t i o n  &  D e p l o y m e n t\\n1.  What ar e quantization and pruning? Ho w do t he y help optimiz e AI models?\\nThese ar e model compr ession t echniques  used to reduce the siz e, latency, and \\npower consumption of deep le arning models — especiall y important for deplo ying \\nmodels on edge de vices lik e smartphones and Io T.\\n🔹  Quantization\\nQuantization r educes the pr ecision of model p ar amet ers (weights and \\nactivations), t ypically from 32-bit flo ating-point (flo at32\\ue082 to 8-bit int egers (int8\\ue082,  \\nwithout signific antly impacting accurac y.\\n✅  Benef it s:\\n•Smaller model siz e\\n•Faster inference (especiall y on mobile/edge de vices)\\n•Reduced memor y usage\\n🛠  T ypes:\\n• P ost -tr aining quantization:  Apply after training ( easy, minimal e xtra steps)\\n• Quantization-aw ar e tr aining \\ue081Q A T\\ue082\\ue092 Simulat es quantization dur ing training  \\n(more accurat e)\\n🔹  Pruning\\nPruning r emoves unnecessar y or less signif ic ant w eight s or neur ons in the \\nnetwork to reduce comput ational o verhead.\\n✅  Benef it s:\\n•Reduces model comple xity\\n•Speeds up inf erence\\n•Saves memor y and po wer\\n🛠  T ypes:\\nQualcomm GenAI Pr eparation\\n1', metadata={'source': 'Interview Questions.pdf', 'page': 0}),\n",
       " Document(page_content='• W eight pruning:  Zero out small w eights\\n• Neur on/channel pruning:  Remove entire neurons or fil ters\\n• Structur ed vs.  unstructur ed: Structur ed pruning maint ains hardware-friendly \\nmodel shape\\n2.  Ho w do y ou deplo y a machine le ar ning model in pr oduction?\\nDeploying a model in volves preparing it for r e al-w or ld use — in web apps, mobile  \\napps, or b ackend services.\\n🚀  St eps t o Deplo y:\\n1. Model Expor t\\n•Convert trained model t o an optimiz ed format: ONNX, T orchScript, \\nSavedModel, or TFLit e.\\n2. Cont ainer ization \\ue081Optional )\\n•Package the model + dependencies using Dock er for reproducibilit y and \\nscalability.\\n3. Ser ving Infr astructur e\\n•Use model ser ving tools:\\n• T ensor Flo w Ser ving\\n• T or chSer v e\\n• T r it on Inf er ence Ser v er\\n• F astAPI/Flask  for custom endpoint s\\n4. Deplo yment\\n•Deploy to:\\n• Cloud\\ue092 AWS Sagemak er, GCP AI Plat form, Azure ML\\n• Edge\\ue092 TensorFlow Lite, ONNX Runtime, NVIDIA T ensorRT\\n• W eb/Mobile \\ue092 TensorFlow.js, CoreML, etc.\\n5. Monit or ing & Logging\\n•Track latency, throughput, input/output dr ift, and er rors.\\nQualcomm GenAI Pr eparation\\n2', metadata={'source': 'Interview Questions.pdf', 'page': 1}),\n",
       " Document(page_content='6. Sc aling\\n•Use Kubernetes or ser verless tools \\ue081AWS Lambda, Google Cloud F unctions)  \\nto auto-scale.\\n3 .  What ar e some challenges in deplo ying lar ge-sc ale AI models?\\n⚠  Common Challenges:\\n1. Lat enc y and Thr oughput\\n•Large models lik e LLMs c an be slo w and require GPUs/TPUs f or real-time \\ninference.\\n2. Memor y Constr aint s\\n•Models ma y not fit int o the memor y of edge de vices or lo w-end ser vers.\\n3. Model V ersioning & CI/CD\\n•Difficult to manage continuous training, t esting, and updating.\\n4. Monit or ing and Dr if t Det ection\\n•Input distr ibution ma y shift over time, degrading model per formance.\\n5. Cost\\n•Running LLMs or ensembles in pr oduction c an be expensive without pr oper \\noptimization.\\n6. Secur it y\\n•Need to prevent model le akage, dat a exposure, and ad versarial attacks.\\n4 .  Ho w do y ou implement model monit or ing and logging in pr oduction?\\nMonitoring ensur es the model beha ves as expected post-deployment and helps  \\ncatch model dr if t,  dat a anomalies , or per f or mance degr adation.\\n✅  What t o Monit or:\\n• Input/output distr ibutions\\n• Pr ediction conf idence\\n• Lat enc y and r esponse time\\n• Syst em r esour ce usage\\nQualcomm GenAI Pr eparation\\n3', metadata={'source': 'Interview Questions.pdf', 'page': 2}),\n",
       " Document(page_content='• Er r or r at e\\n🔧  T ools and F r ame w or ks:\\n• Pr omet heus \\ue09d Gr af ana for dashbo arding\\n• MLflo w for experiment tracking\\n• Evident l y AI for data drift monitoring\\n• Seldon / Bent oML / Fiddler  for model e xplainabilit y & monit oring\\n5 .  What r ole do cont ainer ization t ools lik e Dock er and K uber net es pla y in AI  \\ndeplo yment?\\nT ool Pur pose\\nDock erCreates a por table, reproducible cont ainer with all dependencies\\nbundled f or the model.\\nK uber net es ( K8s)Manages cont ainerized apps at sc ale — suppor ts auto-scaling, fault-\\ntolerance, and r olling updat es.\\n🧩  Wh y The y Mat t er:\\n•Avoid “It worked on m y machine ˮ issues.\\n•Seamlessly deploy and sc ale AI models in distr ibuted environments.\\n•Efficient resource utilization and e asy rollback in case of er rors.\\n6 .  What is ML Ops,  and ho w does it impr o v e AI model lif ec y cle management?\\nML Ops \\ue081Machine Le ar ning Oper ations) is the ML equiv alent of De vOps — a set of  \\npractices t o automate and str eamline the ML lif ecycle.\\n🔁  K e y Phases of ML Ops:\\n1. Dat a v ersioning\\n2. Model tr aining and v alidation\\n3. CI/CD pipelines f or model updat es\\n4. Deplo yment & monit or ing\\n5. Model r ollb ack & go v er nance\\n🧠  T ools In v ol v ed:\\nQualcomm GenAI Pr eparation\\n4', metadata={'source': 'Interview Questions.pdf', 'page': 3}),\n",
       " Document(page_content='• Dat a Management \\ue092 DVC, Delta Lake\\n• Exper iment T r acking\\ue092 MLflow, Weights & Biases\\n• Pipelines \\ue092 Kubeflow, Airflow, TFX\\n• Deplo yment\\ue092 Seldon, Bent oML, SageMak er\\n• Monit or ing\\ue092 Prometheus, Graf ana, Fiddler\\n✅ ML Ops ensur es r epr oducibilit y ,  r eliabilit y ,  and sc alabilit y of machine le arning \\nsystems in pr oduction en vironments.\\nSummar y of Section 5\\nConcept K e y T ak e aw a ys\\nQuantizationReduces model pr ecision ( e.g., float32 \\ue1d7 int8\\ue082 t o improve speed and\\nreduce siz e\\nPruning Removes less useful w eights or neur ons for efficiency\\nDeplo yment\\nPr ocessExport model → cont ainerize → serve → monit or\\nChallenges Latency, memor y, drift, cost, secur ity\\nMonit or ing Track input/output, lat ency, drift, errors\\nML OpsAutomates entire ML lifecycle with CI/CD , versioning, go vernance,\\nmonitoring\\nC l o u d  &  A I  I n f r a s t r u c t u r e\\n1.  What ar e t he k e y dif f er ences bet w een A W S,  GCP ,  and Azur e f or ML  \\nw or klo ads?\\nThese cloud plat forms offer similar ML ser vices but dif fer in ease of use,  \\necosystem integration, and pr icing.\\nF e atur e A W S GCP \\ue081Google Cloud ) Azur e\\nFlagship ML\\nSer viceAmazon SageMak erVertex AIAzure Machine\\nLearning\\nStr engt hsMature ecosyst em,\\nbroad service rangeStrong in AI/ML, dat a\\ntools \\ue081BigQuer y, TPUs)Enterprise integration\\nwith Micr osoft tools\\nQualcomm GenAI Pr eparation\\n5', metadata={'source': 'Interview Questions.pdf', 'page': 4}),\n",
       " Document(page_content='Har dw ar e\\nOptionsGPUs \\ue081A 100, T4\\ue082,\\nInferentia chipsGPUs & TPUs (f or\\nTensorFlow, JAX\\ue082GPUs, int egration\\nwith Azur e IoT\\nDat a T ools S3, Glue, A thena BigQuer y, DataprocAzure Blob St orage,\\nData Lake\\nDe v eloper UXMore configuration-\\nbasedSimpler UI, w ell\\nintegratedAzure Studio, GUI-\\nfriendly\\nCommunit y &\\nA doptionMost adopt ed in\\nenterprisesPopular with AI\\nstartups/researchGrows with e xisting\\nMS client s\\n🧠 Summar y:\\n•Use A W S for flexibility and ent erprise-grade t ools.\\n•Use GCP for AI-first w orkloads (especiall y TensorFlow, JAX\\ue082.\\n•Use Azur e for organizations alr eady on Micr osoft stack.\\n2.  Ho w do y ou sc ale AI w or klo ads on cloud plat f or ms?\\nScaling involves handling lar ger dat aset s,  bigger models,  or mor e users without  \\nperformance degradation.\\n✅  K e y Str at egies:\\n1.  Hor iz ont al Sc aling \\ue081Dat a P ar allelism):\\n•Train mul tiple copies of the model on dif ferent data shards.\\n•Use distr ibuted training librar ies like:\\n• Hor o v od \\ue081TensorFlow, PyTorch)\\n• Py T or ch DDP\\n• R a y\\n2.  V er tic al Sc aling \\ue081Model P ar allelism):\\n•Split a lar ge model acr oss multiple GPUs ( e.g., LLMs that c anʼt fit in one  \\ndevice).\\n•Tools: DeepSpeed , T ensor P ar allel, Megatr on-LM\\n3 .  Aut o-sc aling Inf er ence:\\n•Deploy models with aut o-sc aling endpoint s.\\nQualcomm GenAI Pr eparation\\n6', metadata={'source': 'Interview Questions.pdf', 'page': 5}),\n",
       " Document(page_content='•Use serverless options lik e A W S Lambda , GCP Cloud F unctions , Azur e  \\nF unctions .\\n4 .  K uber net es ( K8s):\\n•Use K ubeFlo w, Seldon, or T r it on Ser v er on Kubernetes clusters for scalable, \\nreproducible ML pipelines.\\n5 .  Spot Inst ances:\\n•Use spot/pr eemptible inst ances for cost-effective training.\\n3 .  Ho w does K uber net es help in deplo ying ML models?\\nK uber net es ( K8s) is an orchestration syst em for automating the deplo yment, \\nscaling, and management of cont ainerized applic ations — including ML models.\\n🔧  K e y Benef it s f or ML\\ue092\\n• Sc alabilit y: Automatically scale up/do wn based on lo ad.\\n• Lo ad Balancing:  Distributes traffic across multiple model inst ances.\\n• R esour ce Isolation:  Allocate specific CPU /GPU to each job.\\n• R olling Updat es: Deploy new models without do wntime.\\n• Job Scheduling:  Run training/inf erence jobs in an or ganized, efficient wa y.\\n🧠 Tools like K ubeFlo w ,  MLflo w ,  A ir flo w ,  Seldon Cor e, and Ar go W or kflo ws \\nintegrate easily with Kubernetes.\\n4 .  What is t he dif f er ence bet w een b at ch and r e al-time inf er ence?\\nF e atur e Bat ch Inf er ence R e al- Time Inf er ence\\nTiming Run on a schedule or in bulk Instant, on-demand\\nLat enc y Seconds t o hours Milliseconds t o seconds\\nUse CasesFraud det ection (overnight), chur n\\npredictionChatbots, voice assist ants, LLMs\\nImplement ationUse job schedulers \\ue081A irflow,\\nSageMak er Batch)Expose models as APIs \\ue081F astAPI,\\nFlask, Triton)\\nCost More cost-efficient at sc ale Higher infrastructur e cost\\nQualcomm GenAI Pr eparation\\n7', metadata={'source': 'Interview Questions.pdf', 'page': 6}),\n",
       " Document(page_content='✅ Choose b at ch when real-time is not r equired and r e al-time when \\nresponsiv eness mat ters (e.g., voice apps, RA G-based agent s).\\n5 .  What ar e t he ad v ant ages of using TPUs o v er GPUs in AI w or klo ads?\\nTPUs \\ue081T ensor Pr ocessing Unit s) are specializ ed hardware developed b y Google  \\nspecifically for deep le arning tasks, especiall y TensorFlow and JAX.\\nA spect TPU GPU\\nPur pose Optimized for matrix ops \\ue081ML\\ue082General-pur pose parallel\\nprocessing\\nP er f or manceBetter throughput f or large-\\nscale trainingStrong performance acr oss\\nframeworks\\nEner gy Ef f icienc yMore efficient for training deep\\nmodelsHigher po wer consumption\\nF r ame w or k\\nComp atibilit yBest with T ensorFlow, JAXCompatible with T ensorFlow,\\nPyTorch, others\\nPr ice \\ue081GCP\\ue082Lower cost-per-comput e for\\ntrainingSlightly higher f or equiv alent\\nthroughput\\n✅ Use TPUs f or high-t hr oughput tr aining in T ensor Flo w/ J AX, and GPUs f or \\nfle xibilit y across frame works (e.g., PyTorch, multi-modal apps).\\n6 .  What t ools do y ou use f or cloud-b ased model tr acking and collabor ation?\\nHere are common t ools used acr oss cloud plat forms to tr ack e xper iment s,  \\nmanage models,  and collabor at e:\\nT ool Pur pose\\nMLflo w Track experiments, log metr ics, manage models\\nW eight s & Biases\\n\\ue081W &B\\ue082Track training, visualiz e metrics, collaborat e\\nT ensor Bo ar d Visualize loss/accurac y, embeddings, graphs\\nD V C \\ue081Dat a\\nV ersion Contr ol )Track dat a, model v ersions, r eproducibilit y\\nQualcomm GenAI Pr eparation\\n8', metadata={'source': 'Interview Questions.pdf', 'page': 7}),\n",
       " Document(page_content='A ir flo w /\\nK ubeFlo w\\nPipelinesDefine and manage comple x ML workflows\\nSageMak er\\nStudio / GCP AI\\nW or kbench /\\nAzur e ML StudioFull cloud-b ased not ebooks with logging and deplo yment\\nNeptune.ai /\\nComet.mlExperiment management and comp arison\\nSummar y of Section 6\\nT opic K e y P oint s\\nCloud Plat f or msAWS \\ue09b fle xibility, GCP \\ue09b AI-nativ e, Azure = enterprise MS int egration\\nSc aling AI Use data/model p arallelism, aut o-scaling, ser verless inference\\nK uber net es Orchestrat es scalable, reproducible ML w orkflows\\nInf er ence\\nModesBatch = bulk pr ocessing; R eal-time = lo w-latency apps\\nTPUs vs GPUs TPUs = f aster for TensorFlow/JAX, GPUs = mor e flexible\\nT r acking &\\nCollabor ationUse MLflo w, W&B, TensorBoard, DVC for efficient w orkflows\\nA g e n t i c  A I  i n  t h e  F i e l d  o f  L L M s\\n✅  What is A gentic AI?\\nA gentic AI  refers to aut onomous,  go al-dr iv en syst ems built using LLMs ( or other \\nAI models) that c an:\\n•Perceive their en vironment (via APIs or t ools),\\n•Reason about the ne xt best action,\\n•Make decisions,\\n•Act (e.g., call functions, generat e content, fetch info),\\n•Reflect and it erate based on out comes.\\nQualcomm GenAI Pr eparation\\n9', metadata={'source': 'Interview Questions.pdf', 'page': 8}),\n",
       " Document(page_content='🧠 Think of them as LLMs \\ue09d Memor y \\ue09d T ools \\ue09d Aut onom y — not just p assive \\nresponders, but activ e, intelligent agent s.\\n🔄  Ho w is it dif f er ent fr om a b asic LLM chat bot?\\nF e atur e Basic LLM A gentic LLM\\nR esponse\\nSt y leOne-shot answ er to a\\npromptMulti-step reasoning, planning, t ool use\\nMemor yStateless (unless e xternally\\nmanaged)Can store, retrieve, and updat e memor y\\nAut onom y Reactive Proactive, goal-oriented\\nT ools/ A ctions Purely text outputCan use t ools/APIs, take actions, c all\\nfunctions\\nEx ampleChatGPT answ ering a\\nquestionAutoGPT planning a r esearch report,\\ncoding it, sending emails\\n🧱  Cor e Component s of A gentic AI Syst ems\\n1.  Planning Module\\n•Breaks down high-le vel goals into smaller t asks.\\n•Can use chain-of -thought pr ompting, t ask trees, or planner models.\\n2.  Memor y Module\\n•Stores previous int eractions, f acts, decisions, and le arned kno wledge.\\n•Can use v ect or dat ab ases (e.g., Chr oma, FAISS, Pinecone ) to recall relevant \\ncontext.\\n•Types of memor y:\\n•Short-term (within a session)\\n•Long-term (persist ent across sessions)\\n•Episodic (f or chronologic al recall)\\n3 .  T ool Use / F unction Calling\\n•Agents can call e x t er nal t ools to accomplish t asks:\\n•Web search\\nQualcomm GenAI Pr eparation\\n10', metadata={'source': 'Interview Questions.pdf', 'page': 9}),\n",
       " Document(page_content='•APIs (e.g., calendar, weather, financial dat a)\\n•Code execution\\n•File syst em access\\n•OpenAIʼs function c alling, LangChain T ools, and LlamaInde x agents enable  \\nthis.\\n4 .  R eflection and I t er ation\\n•Agents reflect on f ailures, revise plans, and r etry intelligently.\\n•Inspired by met a-cognition  — “thinking about thinking. ˮ\\n🔗  P opular F r ame w or ks f or Building A gentic LLMs\\n1.  LangChain\\n•Modular frame work for LLM-po wered agent s.\\n•Key component s:\\n• T ools (search, code, math, APIs)\\n• Memor y (short/long term)\\n• Chains (structur ed workflows)\\n• A gent s (tool-using LLMs)\\n2.  Aut oGPT\\n•Open-sour ce project that str ings together LLM pr ompts to build aut onomous  \\nagents.\\n•Given a goal, it plans, e xecutes, and r evises.\\n•Uses memor y (vector store), file syst em access, w eb tools.\\n3 .  Bab y A GI\\n•Lightweight LLM agent with a t ask list syst em.\\n•Creates, prioritizes, and e xecutes tasks based on a main objectiv e.\\n4 .  LlamaInde x (f or mer l y GPT Inde x)\\n•Focused on int elligent r etrieval.\\n•Powers RAG agents with memor y, context-aware tool use.\\nQualcomm GenAI Pr eparation\\n11', metadata={'source': 'Interview Questions.pdf', 'page': 10}),\n",
       " Document(page_content='⚙  Use Cases of A gentic LLMs\\nDomain Ex ample\\nEnt er pr ise Aut omation Agents that read emails, schedule meetings, generat e reports\\nCust omer Suppor tAgents that underst and full cont ext and take actions lik e refunds\\nDe vOps Agents that monit or logs, identify issues, deplo y fixes\\nCode Gener ation Agents that sc affold projects, test, debug, deplo y\\nR ese ar ch A ssist ant sAgents that gather inf o, summar ize, cross-reference sour ces\\nEduc ation Tutors that adapt t o student pr ogress and personaliz e learning\\n🚨  Challenges and Risks\\nChallenge Descr iption\\nSaf et y and Aut onom y Agents making unint ended decisions\\nHallucination \\ue09d T ool\\nMisuseIncorrect tool usage fr om misunderst ood prompts\\nSc alabilit y Agent loops c an become e xpensive \\ue081API calls, comput ation)\\nSecur it yAccessing and acting upon e xternal systems requires strict\\ncontrol\\nEv aluation Hard to benchmar k agentic beha vior beyond simple metr ics\\n🌱  F utur e Dir ections\\n• Mul ti-agent collabor ation (e.g., “AI teamsˮ working together)\\n• P ersist ent identit y and memor y across long timelines\\n• Emotionall y aw ar e agent s in conversational or therapeutic r oles\\n• H y br id agent s combining symbolic r easoning with LLM fle xibility\\n• Embedded agent s in smar t devices, robotics, Io T\\n🧠  Summar y: A gentic LLMs\\nF e atur e Descr iption\\nDef inition LLMs with memor y, tool use, planning, aut onomy\\nK e y Component sPlanner, memor y, tools, iterative reasoning\\nQualcomm GenAI Pr eparation\\n12', metadata={'source': 'Interview Questions.pdf', 'page': 11}),\n",
       " Document(page_content='P opular F r ame w or ksLangChain, Aut oGPT, BabyAGI, LlamaInde x\\nUse Cases Automation, De vOps, research, customer suppor t\\nChallenges Hallucinations, cost, saf ety, tool misuse\\nT r end Moving toward real-world autonomy and go al-directed intelligence\\nQualcomm GenAI Pr eparation\\n13', metadata={'source': 'Interview Questions.pdf', 'page': 12})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"Interview Questions.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split long texts into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 50)\n",
    "document = splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')\n",
    "vector_base = FAISS.from_documents(document, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Ask a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is AWS?\"\n",
    "docs = vector_base.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate an answer using OpenAI or LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (614 > 512). Running this sequence through the model will result in indexing errors\n",
      "/opt/anaconda3/envs/llmenv/lib/python3.11/site-packages/transformers/pytorch_utils.py:338: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Cloud Plat f or msAWS  fle xibility, GCP  AI-nativ e, Azure = enterprise MS int egration Sc aling AI Use data/model p arallelism, aut o-scaling, ser verless inference • Dat a Management  DVC, Delta Lake • Exper iment T r acking MLflow, Weights & Biases • Pipelines  Kubeflow, Airflow, TFX • Deplo yment Seldon, Bent oML, SageMak er • Monit or ing Prometheus, Graf ana, Fiddler  ML Ops ensur es r epr oducibilit y , r eliabilit y , and sc alabilit y of machine le arning systems in pr oduction en vironments.\n"
     ]
    }
   ],
   "source": [
    "# 5. Use a Free LLM (FLAN-T5) for QA\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# Load FLAN-T5 for generative Q&A\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", max_new_tokens=256)\n",
    "llm = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "# QA Chain (no sources version, suitable for FLAN-T5)\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "response = chain.run(input_documents=docs, question=query)\n",
    "\n",
    "# 6. Output Answer\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
