{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (3.4.1)\n",
      "Requirement already satisfied: langchain in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (0.2.6)\n",
      "Requirement already satisfied: faiss-cpu in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (1.10.0)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (1.35.5)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (4.49.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from sentence-transformers) (2.3.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from sentence-transformers) (0.29.3)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (0.2.10)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (0.1.82)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (2.7.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (8.4.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install sentence-transformers langchain faiss-cpu openai transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load a PDF file or Webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Q u a l c o m m  G e n A I  P r e p a r a t i o n\\nM o d e l  O p t i m i z a t i o n  &  D e p l o y m e n t\\n1.  What ar e quantization and pruning? Ho w do t he y help optimiz e AI models?\\nThese ar e model compr ession t echniques  used to reduce the siz e, latency, and \\npower consumption of deep le arning models â€” especiall y important for deplo ying \\nmodels on edge de vices lik e smartphones and Io T.\\nðŸ”¹  Quantization\\nQuantization r educes the pr ecision of model p ar amet ers (weights and \\nactivations), t ypically from 32-bit flo ating-point (flo at32\\ue082 to 8-bit int egers (int8\\ue082,  \\nwithout signific antly impacting accurac y.\\nâœ…  Benef it s:\\nâ€¢Smaller model siz e\\nâ€¢Faster inference (especiall y on mobile/edge de vices)\\nâ€¢Reduced memor y usage\\nðŸ›   T ypes:\\nâ€¢ P ost -tr aining quantization:  Apply after training ( easy, minimal e xtra steps)\\nâ€¢ Quantization-aw ar e tr aining \\ue081Q A T\\ue082\\ue092 Simulat es quantization dur ing training  \\n(more accurat e)\\nðŸ”¹  Pruning\\nPruning r emoves unnecessar y or less signif ic ant w eight s or neur ons in the \\nnetwork to reduce comput ational o verhead.\\nâœ…  Benef it s:\\nâ€¢Reduces model comple xity\\nâ€¢Speeds up inf erence\\nâ€¢Saves memor y and po wer\\nðŸ›   T ypes:\\nQualcomm GenAI Pr eparation\\n1', metadata={'source': 'Interview Questions.pdf', 'page': 0}),\n",
       " Document(page_content='â€¢ W eight pruning:  Zero out small w eights\\nâ€¢ Neur on/channel pruning:  Remove entire neurons or fil ters\\nâ€¢ Structur ed vs.  unstructur ed: Structur ed pruning maint ains hardware-friendly \\nmodel shape\\n2.  Ho w do y ou deplo y a machine le ar ning model in pr oduction?\\nDeploying a model in volves preparing it for r e al-w or ld use â€” in web apps, mobile  \\napps, or b ackend services.\\nðŸš€  St eps t o Deplo y:\\n1. Model Expor t\\nâ€¢Convert trained model t o an optimiz ed format: ONNX, T orchScript, \\nSavedModel, or TFLit e.\\n2. Cont ainer ization \\ue081Optional )\\nâ€¢Package the model + dependencies using Dock er for reproducibilit y and \\nscalability.\\n3. Ser ving Infr astructur e\\nâ€¢Use model ser ving tools:\\nâ€¢ T ensor Flo w Ser ving\\nâ€¢ T or chSer v e\\nâ€¢ T r it on Inf er ence Ser v er\\nâ€¢ F astAPI/Flask  for custom endpoint s\\n4. Deplo yment\\nâ€¢Deploy to:\\nâ€¢ Cloud\\ue092 AWS Sagemak er, GCP AI Plat form, Azure ML\\nâ€¢ Edge\\ue092 TensorFlow Lite, ONNX Runtime, NVIDIA T ensorRT\\nâ€¢ W eb/Mobile \\ue092 TensorFlow.js, CoreML, etc.\\n5. Monit or ing & Logging\\nâ€¢Track latency, throughput, input/output dr ift, and er rors.\\nQualcomm GenAI Pr eparation\\n2', metadata={'source': 'Interview Questions.pdf', 'page': 1}),\n",
       " Document(page_content='6. Sc aling\\nâ€¢Use Kubernetes or ser verless tools \\ue081AWS Lambda, Google Cloud F unctions)  \\nto auto-scale.\\n3 .  What ar e some challenges in deplo ying lar ge-sc ale AI models?\\nâš   Common Challenges:\\n1. Lat enc y and Thr oughput\\nâ€¢Large models lik e LLMs c an be slo w and require GPUs/TPUs f or real-time \\ninference.\\n2. Memor y Constr aint s\\nâ€¢Models ma y not fit int o the memor y of edge de vices or lo w-end ser vers.\\n3. Model V ersioning & CI/CD\\nâ€¢Difficult to manage continuous training, t esting, and updating.\\n4. Monit or ing and Dr if t Det ection\\nâ€¢Input distr ibution ma y shift over time, degrading model per formance.\\n5. Cost\\nâ€¢Running LLMs or ensembles in pr oduction c an be expensive without pr oper \\noptimization.\\n6. Secur it y\\nâ€¢Need to prevent model le akage, dat a exposure, and ad versarial attacks.\\n4 .  Ho w do y ou implement model monit or ing and logging in pr oduction?\\nMonitoring ensur es the model beha ves as expected post-deployment and helps  \\ncatch model dr if t,  dat a anomalies , or per f or mance degr adation.\\nâœ…  What t o Monit or:\\nâ€¢ Input/output distr ibutions\\nâ€¢ Pr ediction conf idence\\nâ€¢ Lat enc y and r esponse time\\nâ€¢ Syst em r esour ce usage\\nQualcomm GenAI Pr eparation\\n3', metadata={'source': 'Interview Questions.pdf', 'page': 2}),\n",
       " Document(page_content='â€¢ Er r or r at e\\nðŸ”§  T ools and F r ame w or ks:\\nâ€¢ Pr omet heus \\ue09d Gr af ana for dashbo arding\\nâ€¢ MLflo w for experiment tracking\\nâ€¢ Evident l y AI for data drift monitoring\\nâ€¢ Seldon / Bent oML / Fiddler  for model e xplainabilit y & monit oring\\n5 .  What r ole do cont ainer ization t ools lik e Dock er and K uber net es pla y in AI  \\ndeplo yment?\\nT ool Pur pose\\nDock erCreates a por table, reproducible cont ainer with all dependencies\\nbundled f or the model.\\nK uber net es ( K8s)Manages cont ainerized apps at sc ale â€” suppor ts auto-scaling, fault-\\ntolerance, and r olling updat es.\\nðŸ§©  Wh y The y Mat t er:\\nâ€¢Avoid â€œIt worked on m y machine Ë® issues.\\nâ€¢Seamlessly deploy and sc ale AI models in distr ibuted environments.\\nâ€¢Efficient resource utilization and e asy rollback in case of er rors.\\n6 .  What is ML Ops,  and ho w does it impr o v e AI model lif ec y cle management?\\nML Ops \\ue081Machine Le ar ning Oper ations) is the ML equiv alent of De vOps â€” a set of  \\npractices t o automate and str eamline the ML lif ecycle.\\nðŸ”  K e y Phases of ML Ops:\\n1. Dat a v ersioning\\n2. Model tr aining and v alidation\\n3. CI/CD pipelines f or model updat es\\n4. Deplo yment & monit or ing\\n5. Model r ollb ack & go v er nance\\nðŸ§   T ools In v ol v ed:\\nQualcomm GenAI Pr eparation\\n4', metadata={'source': 'Interview Questions.pdf', 'page': 3}),\n",
       " Document(page_content='â€¢ Dat a Management \\ue092 DVC, Delta Lake\\nâ€¢ Exper iment T r acking\\ue092 MLflow, Weights & Biases\\nâ€¢ Pipelines \\ue092 Kubeflow, Airflow, TFX\\nâ€¢ Deplo yment\\ue092 Seldon, Bent oML, SageMak er\\nâ€¢ Monit or ing\\ue092 Prometheus, Graf ana, Fiddler\\nâœ… ML Ops ensur es r epr oducibilit y ,  r eliabilit y ,  and sc alabilit y of machine le arning \\nsystems in pr oduction en vironments.\\nSummar y of Section 5\\nConcept K e y T ak e aw a ys\\nQuantizationReduces model pr ecision ( e.g., float32 \\ue1d7 int8\\ue082 t o improve speed and\\nreduce siz e\\nPruning Removes less useful w eights or neur ons for efficiency\\nDeplo yment\\nPr ocessExport model â†’ cont ainerize â†’ serve â†’ monit or\\nChallenges Latency, memor y, drift, cost, secur ity\\nMonit or ing Track input/output, lat ency, drift, errors\\nML OpsAutomates entire ML lifecycle with CI/CD , versioning, go vernance,\\nmonitoring\\nC l o u d  &  A I  I n f r a s t r u c t u r e\\n1.  What ar e t he k e y dif f er ences bet w een A W S,  GCP ,  and Azur e f or ML  \\nw or klo ads?\\nThese cloud plat forms offer similar ML ser vices but dif fer in ease of use,  \\necosystem integration, and pr icing.\\nF e atur e A W S GCP \\ue081Google Cloud ) Azur e\\nFlagship ML\\nSer viceAmazon SageMak erVertex AIAzure Machine\\nLearning\\nStr engt hsMature ecosyst em,\\nbroad service rangeStrong in AI/ML, dat a\\ntools \\ue081BigQuer y, TPUs)Enterprise integration\\nwith Micr osoft tools\\nQualcomm GenAI Pr eparation\\n5', metadata={'source': 'Interview Questions.pdf', 'page': 4}),\n",
       " Document(page_content='Har dw ar e\\nOptionsGPUs \\ue081A 100, T4\\ue082,\\nInferentia chipsGPUs & TPUs (f or\\nTensorFlow, JAX\\ue082GPUs, int egration\\nwith Azur e IoT\\nDat a T ools S3, Glue, A thena BigQuer y, DataprocAzure Blob St orage,\\nData Lake\\nDe v eloper UXMore configuration-\\nbasedSimpler UI, w ell\\nintegratedAzure Studio, GUI-\\nfriendly\\nCommunit y &\\nA doptionMost adopt ed in\\nenterprisesPopular with AI\\nstartups/researchGrows with e xisting\\nMS client s\\nðŸ§  Summar y:\\nâ€¢Use A W S for flexibility and ent erprise-grade t ools.\\nâ€¢Use GCP for AI-first w orkloads (especiall y TensorFlow, JAX\\ue082.\\nâ€¢Use Azur e for organizations alr eady on Micr osoft stack.\\n2.  Ho w do y ou sc ale AI w or klo ads on cloud plat f or ms?\\nScaling involves handling lar ger dat aset s,  bigger models,  or mor e users without  \\nperformance degradation.\\nâœ…  K e y Str at egies:\\n1.  Hor iz ont al Sc aling \\ue081Dat a P ar allelism):\\nâ€¢Train mul tiple copies of the model on dif ferent data shards.\\nâ€¢Use distr ibuted training librar ies like:\\nâ€¢ Hor o v od \\ue081TensorFlow, PyTorch)\\nâ€¢ Py T or ch DDP\\nâ€¢ R a y\\n2.  V er tic al Sc aling \\ue081Model P ar allelism):\\nâ€¢Split a lar ge model acr oss multiple GPUs ( e.g., LLMs that c anÊ¼t fit in one  \\ndevice).\\nâ€¢Tools: DeepSpeed , T ensor P ar allel, Megatr on-LM\\n3 .  Aut o-sc aling Inf er ence:\\nâ€¢Deploy models with aut o-sc aling endpoint s.\\nQualcomm GenAI Pr eparation\\n6', metadata={'source': 'Interview Questions.pdf', 'page': 5}),\n",
       " Document(page_content='â€¢Use serverless options lik e A W S Lambda , GCP Cloud F unctions , Azur e  \\nF unctions .\\n4 .  K uber net es ( K8s):\\nâ€¢Use K ubeFlo w, Seldon, or T r it on Ser v er on Kubernetes clusters for scalable, \\nreproducible ML pipelines.\\n5 .  Spot Inst ances:\\nâ€¢Use spot/pr eemptible inst ances for cost-effective training.\\n3 .  Ho w does K uber net es help in deplo ying ML models?\\nK uber net es ( K8s) is an orchestration syst em for automating the deplo yment, \\nscaling, and management of cont ainerized applic ations â€” including ML models.\\nðŸ”§  K e y Benef it s f or ML\\ue092\\nâ€¢ Sc alabilit y: Automatically scale up/do wn based on lo ad.\\nâ€¢ Lo ad Balancing:  Distributes traffic across multiple model inst ances.\\nâ€¢ R esour ce Isolation:  Allocate specific CPU /GPU to each job.\\nâ€¢ R olling Updat es: Deploy new models without do wntime.\\nâ€¢ Job Scheduling:  Run training/inf erence jobs in an or ganized, efficient wa y.\\nðŸ§  Tools like K ubeFlo w ,  MLflo w ,  A ir flo w ,  Seldon Cor e, and Ar go W or kflo ws \\nintegrate easily with Kubernetes.\\n4 .  What is t he dif f er ence bet w een b at ch and r e al-time inf er ence?\\nF e atur e Bat ch Inf er ence R e al- Time Inf er ence\\nTiming Run on a schedule or in bulk Instant, on-demand\\nLat enc y Seconds t o hours Milliseconds t o seconds\\nUse CasesFraud det ection (overnight), chur n\\npredictionChatbots, voice assist ants, LLMs\\nImplement ationUse job schedulers \\ue081A irflow,\\nSageMak er Batch)Expose models as APIs \\ue081F astAPI,\\nFlask, Triton)\\nCost More cost-efficient at sc ale Higher infrastructur e cost\\nQualcomm GenAI Pr eparation\\n7', metadata={'source': 'Interview Questions.pdf', 'page': 6}),\n",
       " Document(page_content='âœ… Choose b at ch when real-time is not r equired and r e al-time when \\nresponsiv eness mat ters (e.g., voice apps, RA G-based agent s).\\n5 .  What ar e t he ad v ant ages of using TPUs o v er GPUs in AI w or klo ads?\\nTPUs \\ue081T ensor Pr ocessing Unit s) are specializ ed hardware developed b y Google  \\nspecifically for deep le arning tasks, especiall y TensorFlow and JAX.\\nA spect TPU GPU\\nPur pose Optimized for matrix ops \\ue081ML\\ue082General-pur pose parallel\\nprocessing\\nP er f or manceBetter throughput f or large-\\nscale trainingStrong performance acr oss\\nframeworks\\nEner gy Ef f icienc yMore efficient for training deep\\nmodelsHigher po wer consumption\\nF r ame w or k\\nComp atibilit yBest with T ensorFlow, JAXCompatible with T ensorFlow,\\nPyTorch, others\\nPr ice \\ue081GCP\\ue082Lower cost-per-comput e for\\ntrainingSlightly higher f or equiv alent\\nthroughput\\nâœ… Use TPUs f or high-t hr oughput tr aining in T ensor Flo w/ J AX, and GPUs f or \\nfle xibilit y across frame works (e.g., PyTorch, multi-modal apps).\\n6 .  What t ools do y ou use f or cloud-b ased model tr acking and collabor ation?\\nHere are common t ools used acr oss cloud plat forms to tr ack e xper iment s,  \\nmanage models,  and collabor at e:\\nT ool Pur pose\\nMLflo w Track experiments, log metr ics, manage models\\nW eight s & Biases\\n\\ue081W &B\\ue082Track training, visualiz e metrics, collaborat e\\nT ensor Bo ar d Visualize loss/accurac y, embeddings, graphs\\nD V C \\ue081Dat a\\nV ersion Contr ol )Track dat a, model v ersions, r eproducibilit y\\nQualcomm GenAI Pr eparation\\n8', metadata={'source': 'Interview Questions.pdf', 'page': 7}),\n",
       " Document(page_content='A ir flo w /\\nK ubeFlo w\\nPipelinesDefine and manage comple x ML workflows\\nSageMak er\\nStudio / GCP AI\\nW or kbench /\\nAzur e ML StudioFull cloud-b ased not ebooks with logging and deplo yment\\nNeptune.ai /\\nComet.mlExperiment management and comp arison\\nSummar y of Section 6\\nT opic K e y P oint s\\nCloud Plat f or msAWS \\ue09b fle xibility, GCP \\ue09b AI-nativ e, Azure = enterprise MS int egration\\nSc aling AI Use data/model p arallelism, aut o-scaling, ser verless inference\\nK uber net es Orchestrat es scalable, reproducible ML w orkflows\\nInf er ence\\nModesBatch = bulk pr ocessing; R eal-time = lo w-latency apps\\nTPUs vs GPUs TPUs = f aster for TensorFlow/JAX, GPUs = mor e flexible\\nT r acking &\\nCollabor ationUse MLflo w, W&B, TensorBoard, DVC for efficient w orkflows\\nA g e n t i c  A I  i n  t h e  F i e l d  o f  L L M s\\nâœ…  What is A gentic AI?\\nA gentic AI  refers to aut onomous,  go al-dr iv en syst ems built using LLMs ( or other \\nAI models) that c an:\\nâ€¢Perceive their en vironment (via APIs or t ools),\\nâ€¢Reason about the ne xt best action,\\nâ€¢Make decisions,\\nâ€¢Act (e.g., call functions, generat e content, fetch info),\\nâ€¢Reflect and it erate based on out comes.\\nQualcomm GenAI Pr eparation\\n9', metadata={'source': 'Interview Questions.pdf', 'page': 8}),\n",
       " Document(page_content='ðŸ§  Think of them as LLMs \\ue09d Memor y \\ue09d T ools \\ue09d Aut onom y â€” not just p assive \\nresponders, but activ e, intelligent agent s.\\nðŸ”„  Ho w is it dif f er ent fr om a b asic LLM chat bot?\\nF e atur e Basic LLM A gentic LLM\\nR esponse\\nSt y leOne-shot answ er to a\\npromptMulti-step reasoning, planning, t ool use\\nMemor yStateless (unless e xternally\\nmanaged)Can store, retrieve, and updat e memor y\\nAut onom y Reactive Proactive, goal-oriented\\nT ools/ A ctions Purely text outputCan use t ools/APIs, take actions, c all\\nfunctions\\nEx ampleChatGPT answ ering a\\nquestionAutoGPT planning a r esearch report,\\ncoding it, sending emails\\nðŸ§±  Cor e Component s of A gentic AI Syst ems\\n1.  Planning Module\\nâ€¢Breaks down high-le vel goals into smaller t asks.\\nâ€¢Can use chain-of -thought pr ompting, t ask trees, or planner models.\\n2.  Memor y Module\\nâ€¢Stores previous int eractions, f acts, decisions, and le arned kno wledge.\\nâ€¢Can use v ect or dat ab ases (e.g., Chr oma, FAISS, Pinecone ) to recall relevant \\ncontext.\\nâ€¢Types of memor y:\\nâ€¢Short-term (within a session)\\nâ€¢Long-term (persist ent across sessions)\\nâ€¢Episodic (f or chronologic al recall)\\n3 .  T ool Use / F unction Calling\\nâ€¢Agents can call e x t er nal t ools to accomplish t asks:\\nâ€¢Web search\\nQualcomm GenAI Pr eparation\\n10', metadata={'source': 'Interview Questions.pdf', 'page': 9}),\n",
       " Document(page_content='â€¢APIs (e.g., calendar, weather, financial dat a)\\nâ€¢Code execution\\nâ€¢File syst em access\\nâ€¢OpenAIÊ¼s function c alling, LangChain T ools, and LlamaInde x agents enable  \\nthis.\\n4 .  R eflection and I t er ation\\nâ€¢Agents reflect on f ailures, revise plans, and r etry intelligently.\\nâ€¢Inspired by met a-cognition  â€” â€œthinking about thinking. Ë®\\nðŸ”—  P opular F r ame w or ks f or Building A gentic LLMs\\n1.  LangChain\\nâ€¢Modular frame work for LLM-po wered agent s.\\nâ€¢Key component s:\\nâ€¢ T ools (search, code, math, APIs)\\nâ€¢ Memor y (short/long term)\\nâ€¢ Chains (structur ed workflows)\\nâ€¢ A gent s (tool-using LLMs)\\n2.  Aut oGPT\\nâ€¢Open-sour ce project that str ings together LLM pr ompts to build aut onomous  \\nagents.\\nâ€¢Given a goal, it plans, e xecutes, and r evises.\\nâ€¢Uses memor y (vector store), file syst em access, w eb tools.\\n3 .  Bab y A GI\\nâ€¢Lightweight LLM agent with a t ask list syst em.\\nâ€¢Creates, prioritizes, and e xecutes tasks based on a main objectiv e.\\n4 .  LlamaInde x (f or mer l y GPT Inde x)\\nâ€¢Focused on int elligent r etrieval.\\nâ€¢Powers RAG agents with memor y, context-aware tool use.\\nQualcomm GenAI Pr eparation\\n11', metadata={'source': 'Interview Questions.pdf', 'page': 10}),\n",
       " Document(page_content='âš™  Use Cases of A gentic LLMs\\nDomain Ex ample\\nEnt er pr ise Aut omation Agents that read emails, schedule meetings, generat e reports\\nCust omer Suppor tAgents that underst and full cont ext and take actions lik e refunds\\nDe vOps Agents that monit or logs, identify issues, deplo y fixes\\nCode Gener ation Agents that sc affold projects, test, debug, deplo y\\nR ese ar ch A ssist ant sAgents that gather inf o, summar ize, cross-reference sour ces\\nEduc ation Tutors that adapt t o student pr ogress and personaliz e learning\\nðŸš¨  Challenges and Risks\\nChallenge Descr iption\\nSaf et y and Aut onom y Agents making unint ended decisions\\nHallucination \\ue09d T ool\\nMisuseIncorrect tool usage fr om misunderst ood prompts\\nSc alabilit y Agent loops c an become e xpensive \\ue081API calls, comput ation)\\nSecur it yAccessing and acting upon e xternal systems requires strict\\ncontrol\\nEv aluation Hard to benchmar k agentic beha vior beyond simple metr ics\\nðŸŒ±  F utur e Dir ections\\nâ€¢ Mul ti-agent collabor ation (e.g., â€œAI teamsË® working together)\\nâ€¢ P ersist ent identit y and memor y across long timelines\\nâ€¢ Emotionall y aw ar e agent s in conversational or therapeutic r oles\\nâ€¢ H y br id agent s combining symbolic r easoning with LLM fle xibility\\nâ€¢ Embedded agent s in smar t devices, robotics, Io T\\nðŸ§   Summar y: A gentic LLMs\\nF e atur e Descr iption\\nDef inition LLMs with memor y, tool use, planning, aut onomy\\nK e y Component sPlanner, memor y, tools, iterative reasoning\\nQualcomm GenAI Pr eparation\\n12', metadata={'source': 'Interview Questions.pdf', 'page': 11}),\n",
       " Document(page_content='P opular F r ame w or ksLangChain, Aut oGPT, BabyAGI, LlamaInde x\\nUse Cases Automation, De vOps, research, customer suppor t\\nChallenges Hallucinations, cost, saf ety, tool misuse\\nT r end Moving toward real-world autonomy and go al-directed intelligence\\nQualcomm GenAI Pr eparation\\n13', metadata={'source': 'Interview Questions.pdf', 'page': 12})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"Interview Questions.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split long texts into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 50)\n",
    "document = splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')\n",
    "vector_base = FAISS.from_documents(document, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Ask a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is AWS?\"\n",
    "docs = vector_base.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate an answer using OpenAI or LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (614 > 512). Running this sequence through the model will result in indexing errors\n",
      "/opt/anaconda3/envs/llmenv/lib/python3.11/site-packages/transformers/pytorch_utils.py:338: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Cloud Plat f or msAWS  fle xibility, GCP  AI-nativ e, Azure = enterprise MS int egration Sc aling AI Use data/model p arallelism, aut o-scaling, ser verless inference â€¢ Dat a Management  DVC, Delta Lake â€¢ Exper iment T r acking MLflow, Weights & Biases â€¢ Pipelines  Kubeflow, Airflow, TFX â€¢ Deplo yment Seldon, Bent oML, SageMak er â€¢ Monit or ing Prometheus, Graf ana, Fiddler  ML Ops ensur es r epr oducibilit y , r eliabilit y , and sc alabilit y of machine le arning systems in pr oduction en vironments.\n"
     ]
    }
   ],
   "source": [
    "# 5. Use a Free LLM (FLAN-T5) for QA\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# Load FLAN-T5 for generative Q&A\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", max_new_tokens=256)\n",
    "llm = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "# QA Chain (no sources version, suitable for FLAN-T5)\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "response = chain.run(input_documents=docs, question=query)\n",
    "\n",
    "# 6. Output Answer\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
