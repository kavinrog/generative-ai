{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (3.4.1)\n",
      "Requirement already satisfied: langchain in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (0.2.6)\n",
      "Requirement already satisfied: faiss-cpu in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (1.10.0)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (1.35.5)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (4.49.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from sentence-transformers) (2.3.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from sentence-transformers) (0.29.3)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (0.2.10)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (0.1.82)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (2.7.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain) (8.4.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install sentence-transformers langchain faiss-cpu openai transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load a PDF file or Webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Q u a l c o m m  G e n A I  P r e p a r a t i o n\\nM o d e l  O p t i m i z a t i o n  &  D e p l o y m e n t\\n1.  What ar e quantization and pruning? Ho w do t he y help optimiz e AI models?\\nThese ar e model compr ession t echniques  used to reduce the siz e, latency, and \\npower consumption of deep le arning models ‚Äî especiall y important for deplo ying \\nmodels on edge de vices lik e smartphones and Io T.\\nüîπ  Quantization\\nQuantization r educes the pr ecision of model p ar amet ers (weights and \\nactivations), t ypically from 32-bit flo ating-point (flo at32\\ue082 to 8-bit int egers (int8\\ue082,  \\nwithout signific antly impacting accurac y.\\n‚úÖ  Benef it s:\\n‚Ä¢Smaller model siz e\\n‚Ä¢Faster inference (especiall y on mobile/edge de vices)\\n‚Ä¢Reduced memor y usage\\nüõ†  T ypes:\\n‚Ä¢ P ost -tr aining quantization:  Apply after training ( easy, minimal e xtra steps)\\n‚Ä¢ Quantization-aw ar e tr aining \\ue081Q A T\\ue082\\ue092 Simulat es quantization dur ing training  \\n(more accurat e)\\nüîπ  Pruning\\nPruning r emoves unnecessar y or less signif ic ant w eight s or neur ons in the \\nnetwork to reduce comput ational o verhead.\\n‚úÖ  Benef it s:\\n‚Ä¢Reduces model comple xity\\n‚Ä¢Speeds up inf erence\\n‚Ä¢Saves memor y and po wer\\nüõ†  T ypes:\\nQualcomm GenAI Pr eparation\\n1', metadata={'source': 'Interview Questions.pdf', 'page': 0}),\n",
       " Document(page_content='‚Ä¢ W eight pruning:  Zero out small w eights\\n‚Ä¢ Neur on/channel pruning:  Remove entire neurons or fil ters\\n‚Ä¢ Structur ed vs.  unstructur ed: Structur ed pruning maint ains hardware-friendly \\nmodel shape\\n2.  Ho w do y ou deplo y a machine le ar ning model in pr oduction?\\nDeploying a model in volves preparing it for r e al-w or ld use ‚Äî in web apps, mobile  \\napps, or b ackend services.\\nüöÄ  St eps t o Deplo y:\\n1. Model Expor t\\n‚Ä¢Convert trained model t o an optimiz ed format: ONNX, T orchScript, \\nSavedModel, or TFLit e.\\n2. Cont ainer ization \\ue081Optional )\\n‚Ä¢Package the model + dependencies using Dock er for reproducibilit y and \\nscalability.\\n3. Ser ving Infr astructur e\\n‚Ä¢Use model ser ving tools:\\n‚Ä¢ T ensor Flo w Ser ving\\n‚Ä¢ T or chSer v e\\n‚Ä¢ T r it on Inf er ence Ser v er\\n‚Ä¢ F astAPI/Flask  for custom endpoint s\\n4. Deplo yment\\n‚Ä¢Deploy to:\\n‚Ä¢ Cloud\\ue092 AWS Sagemak er, GCP AI Plat form, Azure ML\\n‚Ä¢ Edge\\ue092 TensorFlow Lite, ONNX Runtime, NVIDIA T ensorRT\\n‚Ä¢ W eb/Mobile \\ue092 TensorFlow.js, CoreML, etc.\\n5. Monit or ing & Logging\\n‚Ä¢Track latency, throughput, input/output dr ift, and er rors.\\nQualcomm GenAI Pr eparation\\n2', metadata={'source': 'Interview Questions.pdf', 'page': 1}),\n",
       " Document(page_content='6. Sc aling\\n‚Ä¢Use Kubernetes or ser verless tools \\ue081AWS Lambda, Google Cloud F unctions)  \\nto auto-scale.\\n3 .  What ar e some challenges in deplo ying lar ge-sc ale AI models?\\n‚ö†  Common Challenges:\\n1. Lat enc y and Thr oughput\\n‚Ä¢Large models lik e LLMs c an be slo w and require GPUs/TPUs f or real-time \\ninference.\\n2. Memor y Constr aint s\\n‚Ä¢Models ma y not fit int o the memor y of edge de vices or lo w-end ser vers.\\n3. Model V ersioning & CI/CD\\n‚Ä¢Difficult to manage continuous training, t esting, and updating.\\n4. Monit or ing and Dr if t Det ection\\n‚Ä¢Input distr ibution ma y shift over time, degrading model per formance.\\n5. Cost\\n‚Ä¢Running LLMs or ensembles in pr oduction c an be expensive without pr oper \\noptimization.\\n6. Secur it y\\n‚Ä¢Need to prevent model le akage, dat a exposure, and ad versarial attacks.\\n4 .  Ho w do y ou implement model monit or ing and logging in pr oduction?\\nMonitoring ensur es the model beha ves as expected post-deployment and helps  \\ncatch model dr if t,  dat a anomalies , or per f or mance degr adation.\\n‚úÖ  What t o Monit or:\\n‚Ä¢ Input/output distr ibutions\\n‚Ä¢ Pr ediction conf idence\\n‚Ä¢ Lat enc y and r esponse time\\n‚Ä¢ Syst em r esour ce usage\\nQualcomm GenAI Pr eparation\\n3', metadata={'source': 'Interview Questions.pdf', 'page': 2}),\n",
       " Document(page_content='‚Ä¢ Er r or r at e\\nüîß  T ools and F r ame w or ks:\\n‚Ä¢ Pr omet heus \\ue09d Gr af ana for dashbo arding\\n‚Ä¢ MLflo w for experiment tracking\\n‚Ä¢ Evident l y AI for data drift monitoring\\n‚Ä¢ Seldon / Bent oML / Fiddler  for model e xplainabilit y & monit oring\\n5 .  What r ole do cont ainer ization t ools lik e Dock er and K uber net es pla y in AI  \\ndeplo yment?\\nT ool Pur pose\\nDock erCreates a por table, reproducible cont ainer with all dependencies\\nbundled f or the model.\\nK uber net es ( K8s)Manages cont ainerized apps at sc ale ‚Äî suppor ts auto-scaling, fault-\\ntolerance, and r olling updat es.\\nüß©  Wh y The y Mat t er:\\n‚Ä¢Avoid ‚ÄúIt worked on m y machine ÀÆ issues.\\n‚Ä¢Seamlessly deploy and sc ale AI models in distr ibuted environments.\\n‚Ä¢Efficient resource utilization and e asy rollback in case of er rors.\\n6 .  What is ML Ops,  and ho w does it impr o v e AI model lif ec y cle management?\\nML Ops \\ue081Machine Le ar ning Oper ations) is the ML equiv alent of De vOps ‚Äî a set of  \\npractices t o automate and str eamline the ML lif ecycle.\\nüîÅ  K e y Phases of ML Ops:\\n1. Dat a v ersioning\\n2. Model tr aining and v alidation\\n3. CI/CD pipelines f or model updat es\\n4. Deplo yment & monit or ing\\n5. Model r ollb ack & go v er nance\\nüß†  T ools In v ol v ed:\\nQualcomm GenAI Pr eparation\\n4', metadata={'source': 'Interview Questions.pdf', 'page': 3}),\n",
       " Document(page_content='‚Ä¢ Dat a Management \\ue092 DVC, Delta Lake\\n‚Ä¢ Exper iment T r acking\\ue092 MLflow, Weights & Biases\\n‚Ä¢ Pipelines \\ue092 Kubeflow, Airflow, TFX\\n‚Ä¢ Deplo yment\\ue092 Seldon, Bent oML, SageMak er\\n‚Ä¢ Monit or ing\\ue092 Prometheus, Graf ana, Fiddler\\n‚úÖ ML Ops ensur es r epr oducibilit y ,  r eliabilit y ,  and sc alabilit y of machine le arning \\nsystems in pr oduction en vironments.\\nSummar y of Section 5\\nConcept K e y T ak e aw a ys\\nQuantizationReduces model pr ecision ( e.g., float32 \\ue1d7 int8\\ue082 t o improve speed and\\nreduce siz e\\nPruning Removes less useful w eights or neur ons for efficiency\\nDeplo yment\\nPr ocessExport model ‚Üí cont ainerize ‚Üí serve ‚Üí monit or\\nChallenges Latency, memor y, drift, cost, secur ity\\nMonit or ing Track input/output, lat ency, drift, errors\\nML OpsAutomates entire ML lifecycle with CI/CD , versioning, go vernance,\\nmonitoring\\nC l o u d  &  A I  I n f r a s t r u c t u r e\\n1.  What ar e t he k e y dif f er ences bet w een A W S,  GCP ,  and Azur e f or ML  \\nw or klo ads?\\nThese cloud plat forms offer similar ML ser vices but dif fer in ease of use,  \\necosystem integration, and pr icing.\\nF e atur e A W S GCP \\ue081Google Cloud ) Azur e\\nFlagship ML\\nSer viceAmazon SageMak erVertex AIAzure Machine\\nLearning\\nStr engt hsMature ecosyst em,\\nbroad service rangeStrong in AI/ML, dat a\\ntools \\ue081BigQuer y, TPUs)Enterprise integration\\nwith Micr osoft tools\\nQualcomm GenAI Pr eparation\\n5', metadata={'source': 'Interview Questions.pdf', 'page': 4}),\n",
       " Document(page_content='Har dw ar e\\nOptionsGPUs \\ue081A 100, T4\\ue082,\\nInferentia chipsGPUs & TPUs (f or\\nTensorFlow, JAX\\ue082GPUs, int egration\\nwith Azur e IoT\\nDat a T ools S3, Glue, A thena BigQuer y, DataprocAzure Blob St orage,\\nData Lake\\nDe v eloper UXMore configuration-\\nbasedSimpler UI, w ell\\nintegratedAzure Studio, GUI-\\nfriendly\\nCommunit y &\\nA doptionMost adopt ed in\\nenterprisesPopular with AI\\nstartups/researchGrows with e xisting\\nMS client s\\nüß† Summar y:\\n‚Ä¢Use A W S for flexibility and ent erprise-grade t ools.\\n‚Ä¢Use GCP for AI-first w orkloads (especiall y TensorFlow, JAX\\ue082.\\n‚Ä¢Use Azur e for organizations alr eady on Micr osoft stack.\\n2.  Ho w do y ou sc ale AI w or klo ads on cloud plat f or ms?\\nScaling involves handling lar ger dat aset s,  bigger models,  or mor e users without  \\nperformance degradation.\\n‚úÖ  K e y Str at egies:\\n1.  Hor iz ont al Sc aling \\ue081Dat a P ar allelism):\\n‚Ä¢Train mul tiple copies of the model on dif ferent data shards.\\n‚Ä¢Use distr ibuted training librar ies like:\\n‚Ä¢ Hor o v od \\ue081TensorFlow, PyTorch)\\n‚Ä¢ Py T or ch DDP\\n‚Ä¢ R a y\\n2.  V er tic al Sc aling \\ue081Model P ar allelism):\\n‚Ä¢Split a lar ge model acr oss multiple GPUs ( e.g., LLMs that c an ºt fit in one  \\ndevice).\\n‚Ä¢Tools: DeepSpeed , T ensor P ar allel, Megatr on-LM\\n3 .  Aut o-sc aling Inf er ence:\\n‚Ä¢Deploy models with aut o-sc aling endpoint s.\\nQualcomm GenAI Pr eparation\\n6', metadata={'source': 'Interview Questions.pdf', 'page': 5}),\n",
       " Document(page_content='‚Ä¢Use serverless options lik e A W S Lambda , GCP Cloud F unctions , Azur e  \\nF unctions .\\n4 .  K uber net es ( K8s):\\n‚Ä¢Use K ubeFlo w, Seldon, or T r it on Ser v er on Kubernetes clusters for scalable, \\nreproducible ML pipelines.\\n5 .  Spot Inst ances:\\n‚Ä¢Use spot/pr eemptible inst ances for cost-effective training.\\n3 .  Ho w does K uber net es help in deplo ying ML models?\\nK uber net es ( K8s) is an orchestration syst em for automating the deplo yment, \\nscaling, and management of cont ainerized applic ations ‚Äî including ML models.\\nüîß  K e y Benef it s f or ML\\ue092\\n‚Ä¢ Sc alabilit y: Automatically scale up/do wn based on lo ad.\\n‚Ä¢ Lo ad Balancing:  Distributes traffic across multiple model inst ances.\\n‚Ä¢ R esour ce Isolation:  Allocate specific CPU /GPU to each job.\\n‚Ä¢ R olling Updat es: Deploy new models without do wntime.\\n‚Ä¢ Job Scheduling:  Run training/inf erence jobs in an or ganized, efficient wa y.\\nüß† Tools like K ubeFlo w ,  MLflo w ,  A ir flo w ,  Seldon Cor e, and Ar go W or kflo ws \\nintegrate easily with Kubernetes.\\n4 .  What is t he dif f er ence bet w een b at ch and r e al-time inf er ence?\\nF e atur e Bat ch Inf er ence R e al- Time Inf er ence\\nTiming Run on a schedule or in bulk Instant, on-demand\\nLat enc y Seconds t o hours Milliseconds t o seconds\\nUse CasesFraud det ection (overnight), chur n\\npredictionChatbots, voice assist ants, LLMs\\nImplement ationUse job schedulers \\ue081A irflow,\\nSageMak er Batch)Expose models as APIs \\ue081F astAPI,\\nFlask, Triton)\\nCost More cost-efficient at sc ale Higher infrastructur e cost\\nQualcomm GenAI Pr eparation\\n7', metadata={'source': 'Interview Questions.pdf', 'page': 6}),\n",
       " Document(page_content='‚úÖ Choose b at ch when real-time is not r equired and r e al-time when \\nresponsiv eness mat ters (e.g., voice apps, RA G-based agent s).\\n5 .  What ar e t he ad v ant ages of using TPUs o v er GPUs in AI w or klo ads?\\nTPUs \\ue081T ensor Pr ocessing Unit s) are specializ ed hardware developed b y Google  \\nspecifically for deep le arning tasks, especiall y TensorFlow and JAX.\\nA spect TPU GPU\\nPur pose Optimized for matrix ops \\ue081ML\\ue082General-pur pose parallel\\nprocessing\\nP er f or manceBetter throughput f or large-\\nscale trainingStrong performance acr oss\\nframeworks\\nEner gy Ef f icienc yMore efficient for training deep\\nmodelsHigher po wer consumption\\nF r ame w or k\\nComp atibilit yBest with T ensorFlow, JAXCompatible with T ensorFlow,\\nPyTorch, others\\nPr ice \\ue081GCP\\ue082Lower cost-per-comput e for\\ntrainingSlightly higher f or equiv alent\\nthroughput\\n‚úÖ Use TPUs f or high-t hr oughput tr aining in T ensor Flo w/ J AX, and GPUs f or \\nfle xibilit y across frame works (e.g., PyTorch, multi-modal apps).\\n6 .  What t ools do y ou use f or cloud-b ased model tr acking and collabor ation?\\nHere are common t ools used acr oss cloud plat forms to tr ack e xper iment s,  \\nmanage models,  and collabor at e:\\nT ool Pur pose\\nMLflo w Track experiments, log metr ics, manage models\\nW eight s & Biases\\n\\ue081W &B\\ue082Track training, visualiz e metrics, collaborat e\\nT ensor Bo ar d Visualize loss/accurac y, embeddings, graphs\\nD V C \\ue081Dat a\\nV ersion Contr ol )Track dat a, model v ersions, r eproducibilit y\\nQualcomm GenAI Pr eparation\\n8', metadata={'source': 'Interview Questions.pdf', 'page': 7}),\n",
       " Document(page_content='A ir flo w /\\nK ubeFlo w\\nPipelinesDefine and manage comple x ML workflows\\nSageMak er\\nStudio / GCP AI\\nW or kbench /\\nAzur e ML StudioFull cloud-b ased not ebooks with logging and deplo yment\\nNeptune.ai /\\nComet.mlExperiment management and comp arison\\nSummar y of Section 6\\nT opic K e y P oint s\\nCloud Plat f or msAWS \\ue09b fle xibility, GCP \\ue09b AI-nativ e, Azure = enterprise MS int egration\\nSc aling AI Use data/model p arallelism, aut o-scaling, ser verless inference\\nK uber net es Orchestrat es scalable, reproducible ML w orkflows\\nInf er ence\\nModesBatch = bulk pr ocessing; R eal-time = lo w-latency apps\\nTPUs vs GPUs TPUs = f aster for TensorFlow/JAX, GPUs = mor e flexible\\nT r acking &\\nCollabor ationUse MLflo w, W&B, TensorBoard, DVC for efficient w orkflows\\nA g e n t i c  A I  i n  t h e  F i e l d  o f  L L M s\\n‚úÖ  What is A gentic AI?\\nA gentic AI  refers to aut onomous,  go al-dr iv en syst ems built using LLMs ( or other \\nAI models) that c an:\\n‚Ä¢Perceive their en vironment (via APIs or t ools),\\n‚Ä¢Reason about the ne xt best action,\\n‚Ä¢Make decisions,\\n‚Ä¢Act (e.g., call functions, generat e content, fetch info),\\n‚Ä¢Reflect and it erate based on out comes.\\nQualcomm GenAI Pr eparation\\n9', metadata={'source': 'Interview Questions.pdf', 'page': 8}),\n",
       " Document(page_content='üß† Think of them as LLMs \\ue09d Memor y \\ue09d T ools \\ue09d Aut onom y ‚Äî not just p assive \\nresponders, but activ e, intelligent agent s.\\nüîÑ  Ho w is it dif f er ent fr om a b asic LLM chat bot?\\nF e atur e Basic LLM A gentic LLM\\nR esponse\\nSt y leOne-shot answ er to a\\npromptMulti-step reasoning, planning, t ool use\\nMemor yStateless (unless e xternally\\nmanaged)Can store, retrieve, and updat e memor y\\nAut onom y Reactive Proactive, goal-oriented\\nT ools/ A ctions Purely text outputCan use t ools/APIs, take actions, c all\\nfunctions\\nEx ampleChatGPT answ ering a\\nquestionAutoGPT planning a r esearch report,\\ncoding it, sending emails\\nüß±  Cor e Component s of A gentic AI Syst ems\\n1.  Planning Module\\n‚Ä¢Breaks down high-le vel goals into smaller t asks.\\n‚Ä¢Can use chain-of -thought pr ompting, t ask trees, or planner models.\\n2.  Memor y Module\\n‚Ä¢Stores previous int eractions, f acts, decisions, and le arned kno wledge.\\n‚Ä¢Can use v ect or dat ab ases (e.g., Chr oma, FAISS, Pinecone ) to recall relevant \\ncontext.\\n‚Ä¢Types of memor y:\\n‚Ä¢Short-term (within a session)\\n‚Ä¢Long-term (persist ent across sessions)\\n‚Ä¢Episodic (f or chronologic al recall)\\n3 .  T ool Use / F unction Calling\\n‚Ä¢Agents can call e x t er nal t ools to accomplish t asks:\\n‚Ä¢Web search\\nQualcomm GenAI Pr eparation\\n10', metadata={'source': 'Interview Questions.pdf', 'page': 9}),\n",
       " Document(page_content='‚Ä¢APIs (e.g., calendar, weather, financial dat a)\\n‚Ä¢Code execution\\n‚Ä¢File syst em access\\n‚Ä¢OpenAI ºs function c alling, LangChain T ools, and LlamaInde x agents enable  \\nthis.\\n4 .  R eflection and I t er ation\\n‚Ä¢Agents reflect on f ailures, revise plans, and r etry intelligently.\\n‚Ä¢Inspired by met a-cognition  ‚Äî ‚Äúthinking about thinking. ÀÆ\\nüîó  P opular F r ame w or ks f or Building A gentic LLMs\\n1.  LangChain\\n‚Ä¢Modular frame work for LLM-po wered agent s.\\n‚Ä¢Key component s:\\n‚Ä¢ T ools (search, code, math, APIs)\\n‚Ä¢ Memor y (short/long term)\\n‚Ä¢ Chains (structur ed workflows)\\n‚Ä¢ A gent s (tool-using LLMs)\\n2.  Aut oGPT\\n‚Ä¢Open-sour ce project that str ings together LLM pr ompts to build aut onomous  \\nagents.\\n‚Ä¢Given a goal, it plans, e xecutes, and r evises.\\n‚Ä¢Uses memor y (vector store), file syst em access, w eb tools.\\n3 .  Bab y A GI\\n‚Ä¢Lightweight LLM agent with a t ask list syst em.\\n‚Ä¢Creates, prioritizes, and e xecutes tasks based on a main objectiv e.\\n4 .  LlamaInde x (f or mer l y GPT Inde x)\\n‚Ä¢Focused on int elligent r etrieval.\\n‚Ä¢Powers RAG agents with memor y, context-aware tool use.\\nQualcomm GenAI Pr eparation\\n11', metadata={'source': 'Interview Questions.pdf', 'page': 10}),\n",
       " Document(page_content='‚öô  Use Cases of A gentic LLMs\\nDomain Ex ample\\nEnt er pr ise Aut omation Agents that read emails, schedule meetings, generat e reports\\nCust omer Suppor tAgents that underst and full cont ext and take actions lik e refunds\\nDe vOps Agents that monit or logs, identify issues, deplo y fixes\\nCode Gener ation Agents that sc affold projects, test, debug, deplo y\\nR ese ar ch A ssist ant sAgents that gather inf o, summar ize, cross-reference sour ces\\nEduc ation Tutors that adapt t o student pr ogress and personaliz e learning\\nüö®  Challenges and Risks\\nChallenge Descr iption\\nSaf et y and Aut onom y Agents making unint ended decisions\\nHallucination \\ue09d T ool\\nMisuseIncorrect tool usage fr om misunderst ood prompts\\nSc alabilit y Agent loops c an become e xpensive \\ue081API calls, comput ation)\\nSecur it yAccessing and acting upon e xternal systems requires strict\\ncontrol\\nEv aluation Hard to benchmar k agentic beha vior beyond simple metr ics\\nüå±  F utur e Dir ections\\n‚Ä¢ Mul ti-agent collabor ation (e.g., ‚ÄúAI teamsÀÆ working together)\\n‚Ä¢ P ersist ent identit y and memor y across long timelines\\n‚Ä¢ Emotionall y aw ar e agent s in conversational or therapeutic r oles\\n‚Ä¢ H y br id agent s combining symbolic r easoning with LLM fle xibility\\n‚Ä¢ Embedded agent s in smar t devices, robotics, Io T\\nüß†  Summar y: A gentic LLMs\\nF e atur e Descr iption\\nDef inition LLMs with memor y, tool use, planning, aut onomy\\nK e y Component sPlanner, memor y, tools, iterative reasoning\\nQualcomm GenAI Pr eparation\\n12', metadata={'source': 'Interview Questions.pdf', 'page': 11}),\n",
       " Document(page_content='P opular F r ame w or ksLangChain, Aut oGPT, BabyAGI, LlamaInde x\\nUse Cases Automation, De vOps, research, customer suppor t\\nChallenges Hallucinations, cost, saf ety, tool misuse\\nT r end Moving toward real-world autonomy and go al-directed intelligence\\nQualcomm GenAI Pr eparation\\n13', metadata={'source': 'Interview Questions.pdf', 'page': 12})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"Interview Questions.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split long texts into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 50)\n",
    "document = splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x120325ad0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceBgeEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')\n",
    "vector_base = FAISS.from_documents(document, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Ask a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is AWS?\"\n",
    "docs = vector_base.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate an answer using OpenAI or LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/opt/anaconda3/envs/llmenv/lib/python3.11/site-packages/transformers/pytorch_utils.py:338: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "A ir flo w /\n",
      "K ubeFlo w\n",
      "PipelinesDefine and manage comple x ML workflows\n",
      "SageMak er\n",
      "Studio / GCP AI\n",
      "W or kbench /\n",
      "Azur e ML StudioFull cloud-b ased not ebooks with logging and deplo yment\n",
      "Neptune.ai /\n",
      "Comet.mlExperiment management and comp arison\n",
      "Summar y of Section 6\n",
      "T opic K e y P oint s\n",
      "Cloud Plat f or msAWS ÓÇõ fle xibility, GCP ÓÇõ AI-nativ e, Azure = enterprise MS int egration\n",
      "Sc aling AI Use data/model p arallelism, aut o-scaling, ser verless inference\n",
      "\n",
      "‚Ä¢ Dat a Management ÓÇí DVC, Delta Lake\n",
      "‚Ä¢ Exper iment T r ackingÓÇí MLflow, Weights & Biases\n",
      "‚Ä¢ Pipelines ÓÇí Kubeflow, Airflow, TFX\n",
      "‚Ä¢ Deplo ymentÓÇí Seldon, Bent oML, SageMak er\n",
      "‚Ä¢ Monit or ingÓÇí Prometheus, Graf ana, Fiddler\n",
      "‚úÖ ML Ops ensur es r epr oducibilit y ,  r eliabilit y ,  and sc alabilit y of machine le arning \n",
      "systems in pr oduction en vironments.\n",
      "Summar y of Section 5\n",
      "Concept K e y T ak e aw a ys\n",
      "QuantizationReduces model pr ecision ( e.g., float32 Óáó int8ÓÇÇ t o improve speed and\n",
      "reduce siz e\n",
      "\n",
      "w or klo ads?\n",
      "These cloud plat forms offer similar ML ser vices but dif fer in ease of use,  \n",
      "ecosystem integration, and pr icing.\n",
      "F e atur e A W S GCP ÓÇÅGoogle Cloud ) Azur e\n",
      "Flagship ML\n",
      "Ser viceAmazon SageMak erVertex AIAzure Machine\n",
      "Learning\n",
      "Str engt hsMature ecosyst em,\n",
      "broad service rangeStrong in AI/ML, dat a\n",
      "tools ÓÇÅBigQuer y, TPUs)Enterprise integration\n",
      "with Micr osoft tools\n",
      "Qualcomm GenAI Pr eparation\n",
      "5\n",
      "\n",
      "Question: What is AWS?\n",
      "Helpful Answer: As a large dat a nyaa nyaa nyaa nyoo w i n s, the AWS world consists of over a billion ÓÄáa ns\n",
      "\n",
      "namespaces. The name the company is using is the same as the company name or logo used for the product. The last word ÓÄád is the same abbreviation used by many web ÓÄÄs and ÓÄÄs around the world, ÓÄÄd, to indicate the web ÓÄÄs as\n",
      "\n",
      "completed or completed. Amazon ÓÄÄa ns was created through use of\n",
      "\n",
      "Amazon's existing AWS product.\n",
      "\n",
      "The name of AWS was chosen because it embodies a set of basic and\n",
      "\n",
      "functional needs for enterprise customers and serves as the foundation\n",
      "\n",
      "for the products that go into the Amazon services\n",
      "\n",
      "service.\n",
      "\n",
      "The best of The Amazon family of businesses is made ‚Äã‚Äãto fit the needs of a large\n",
      "\n",
      "app. This means the\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# Load GPT-2 via HuggingFace Transformers\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\", max_new_tokens=200)\n",
    "\n",
    "# Wrap in LangChain's LLM interface\n",
    "llm = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "# Load the basic QA chain (note: no \"with_sources\" variant for HuggingFace models)\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "# Run QA with your retrieved documents\n",
    "response = chain.run(input_documents=docs, question=query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
